{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-dual-cnn-tumor_prediction-sites-central-google-cola.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "conda_tensorflow_p36",
      "language": "python",
      "name": "conda_tensorflow_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stomioka/phuse-tumor-ml/blob/master/07_dual_cnn_tumor_prediction_sites_central_google_cola.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eNny0V6f0qdU"
      },
      "source": [
        "# Response criteria prediction for tumor with parallel-CNN\n",
        "\n",
        "Sam Tomioka<br>\n",
        "2019-10-13\n",
        "\n",
        "Same data used in [notebook3](03-tumor_prediction-sites-central.ipynb) will be used here. \n",
        "\n",
        "- Model based on `central`+`site` with 85% of data from each. Test on remaining `central` assessments, Test on remaining `site` assessments independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vvowRGjB0qdX",
        "outputId": "4b13d89b-711a-46fc-c841-2b1bc9db67dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#!pip install git+git://github.com/andirs/impyte.git\n",
        "#!pip install xgboost \n",
        "\n",
        "!git clone https://github.com/stomioka/phuse-tumor-ml.git\n",
        "!pip install git+git://github.com/andirs/impyte.git\n",
        "!mv phuse-tumor-ml phuse_tumor_ml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'phuse-tumor-ml'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 118 (delta 56), reused 79 (delta 22), pack-reused 0\n",
            "Receiving objects: 100% (118/118), 3.48 MiB | 7.05 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Collecting git+git://github.com/andirs/impyte.git\n",
            "  Cloning git://github.com/andirs/impyte.git to /tmp/pip-req-build-e4i8tx8p\n",
            "  Running command git clone -q git://github.com/andirs/impyte.git /tmp/pip-req-build-e4i8tx8p\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from impyte==0.1.0) (0.21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from impyte==0.1.0) (0.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from impyte==0.1.0) (1.16.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from impyte==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from impyte==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->impyte==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->impyte==0.1.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->impyte==0.1.0) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->impyte==0.1.0) (1.12.0)\n",
            "Building wheels for collected packages: impyte\n",
            "  Building wheel for impyte (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for impyte: filename=impyte-0.1.0-cp36-none-any.whl size=21388 sha256=5ec16c677856070dc7d5308d2abdb8b36e2648d0c621b24cf3ad1ab81256dcb9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mbmcd4sj/wheels/65/16/30/1a24d053bf050146af36c12fdca5e3f2362d892226909931e4\n",
            "Successfully built impyte\n",
            "Installing collected packages: impyte\n",
            "Successfully installed impyte-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sFHjN5wH0qdb",
        "outputId": "49cff90b-5323-48a2-cadd-ab0fd4989594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import os\n",
        "os.chdir('phuse_tumor_ml/notebooks')\n",
        "from lib.myutil import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.train import *\n",
        "print('tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score,roc_curve, auc\n",
        "print('sklearn version: {}'.format(sklearn.__version__))\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "#tf.keras.backend.clear_session() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensorflow version: 1.15.0-rc3\n",
            "sklearn version: 0.21.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bW6oQ5SN0qdj",
        "colab": {}
      },
      "source": [
        "central, site=load_data()\n",
        "\n",
        "tr_x, tr_y, ts_x, ts_y, ts_x2, ts_y2 = generate_tr_ts(df1=central, df2=site, m=3, method=None, h=3000, seed=2019)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hzaj0m-n0qdo",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "encoder = LabelBinarizer()\n",
        "tr_y = encoder.fit_transform(tr_y)\n",
        "ts_y = encoder.fit_transform(ts_y)\n",
        "ts_y2 = encoder.fit_transform(ts_y2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xnzgFYj0qd0",
        "outputId": "d20bd2f3-5f61-4ebf-bbd8-603043d42535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tr_x.shape, tr_y.shape\n",
        "x_train, x_vl, y_train, y_vl = train_test_split(tr_x, tr_y, test_size=0.20, random_state=2019)\n",
        "print()\n",
        "x_train=np.array(x_train).reshape(x_train.shape[0],7,1)\n",
        "#y_train=np.array(y_train).reshape(y_train.shape[0],5,1)\n",
        "x_vl=np.array(x_vl).reshape(x_vl.shape[0],7,1)\n",
        "#y_vl=np.array(y_vl).reshape(y_vl.shape[0],5,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WnTEmXDl0qd4",
        "outputId": "34caf640-360d-4f1e-d2b8-b29bded91df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "tf.keras.backend.clear_session() \n",
        "i = Input(shape=(7,1),name='recest')\n",
        "\n",
        "x0 = Conv1D(64, 3, activation='relu', name='1Dconv_1')(i)\n",
        "x0 = Conv1D(64, 3, activation='relu', name='1Dconv_2')(x0)\n",
        "x0 = Dropout(0.5, name='1Dropout')(x0)\n",
        "x0 = MaxPooling1D(pool_size=2, name='1maxpool')(x0)\n",
        "x0 = Flatten()(x0)\n",
        "x0 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l=0.01), name='1out')(x0)\n",
        "\n",
        "x1 = Conv1D(128, 3, activation='relu', name='2Dconv_1')(i)\n",
        "x1 = Conv1D(128, 3, activation='relu', name='2Dconv_2')(x1)\n",
        "x1 = Dropout(0.5, name='2Dropout')(x1)\n",
        "x1 = MaxPooling1D(pool_size=2, name='2maxpool')(x1)\n",
        "x1 = Flatten()(x1)\n",
        "x1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l=0.01), name='2out')(x1)\n",
        "\n",
        "\n",
        "#x1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l=0.01), name='dense_2')(i)\n",
        "x2 = Add()([x0, x1])\n",
        "x2 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l=0.01), name='dense_1')(x2)\n",
        "#x2 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l=0.01), name='dense_2')(x2)\n",
        "#x2 = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(l=0.01), name='dense_3')(x2)\n",
        "x2 = Flatten()(x2)\n",
        "outputs = Dense(5, activation='softmax', name='pred')(x2)\n",
        "\n",
        "model = Model(i, outputs)\n",
        "opt = AdamOptimizer(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "recest (InputLayer)             [(None, 7, 1)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "1Dconv_1 (Conv1D)               (None, 5, 64)        256         recest[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "2Dconv_1 (Conv1D)               (None, 5, 128)       512         recest[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "1Dconv_2 (Conv1D)               (None, 3, 64)        12352       1Dconv_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "2Dconv_2 (Conv1D)               (None, 3, 128)       49280       2Dconv_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "1Dropout (Dropout)              (None, 3, 64)        0           1Dconv_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "2Dropout (Dropout)              (None, 3, 128)       0           2Dconv_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "1maxpool (MaxPooling1D)         (None, 1, 64)        0           1Dropout[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "2maxpool (MaxPooling1D)         (None, 1, 128)       0           2Dropout[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           1maxpool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 128)          0           2maxpool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "1out (Dense)                    (None, 64)           4160        flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "2out (Dense)                    (None, 64)           8256        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 64)           0           1out[0][0]                       \n",
            "                                                                 2out[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           4160        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 64)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pred (Dense)                    (None, 5)            325         flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 79,301\n",
            "Trainable params: 79,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SUsrNxCS0qd8",
        "outputId": "9c0823d2-e198-4225-f2b9-db9850fd1eb0",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=16,\n",
        "                    epochs=500,\n",
        "\n",
        "                    validation_data=(x_vl, y_vl))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1098 samples, validate on 275 samples\n",
            "Epoch 1/500\n",
            "1098/1098 [==============================] - 1s 880us/sample - loss: 2.5039 - acc: 0.6066 - val_loss: 1.7168 - val_acc: 0.7455\n",
            "Epoch 2/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 1.4421 - acc: 0.7313 - val_loss: 1.2473 - val_acc: 0.7382\n",
            "Epoch 3/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 1.1073 - acc: 0.7568 - val_loss: 1.0322 - val_acc: 0.7636\n",
            "Epoch 4/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.9335 - acc: 0.7668 - val_loss: 0.8757 - val_acc: 0.7964\n",
            "Epoch 5/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.8225 - acc: 0.7860 - val_loss: 0.8079 - val_acc: 0.8036\n",
            "Epoch 6/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.7499 - acc: 0.8169 - val_loss: 0.7396 - val_acc: 0.8218\n",
            "Epoch 7/500\n",
            "1098/1098 [==============================] - 0s 268us/sample - loss: 0.7094 - acc: 0.8069 - val_loss: 0.7239 - val_acc: 0.8036\n",
            "Epoch 8/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.6455 - acc: 0.8306 - val_loss: 0.6529 - val_acc: 0.8436\n",
            "Epoch 9/500\n",
            "1098/1098 [==============================] - 0s 267us/sample - loss: 0.6072 - acc: 0.8297 - val_loss: 0.6056 - val_acc: 0.8582\n",
            "Epoch 10/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.5926 - acc: 0.8452 - val_loss: 0.5847 - val_acc: 0.8691\n",
            "Epoch 11/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.5624 - acc: 0.8534 - val_loss: 0.5668 - val_acc: 0.8473\n",
            "Epoch 12/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.5552 - acc: 0.8561 - val_loss: 0.5644 - val_acc: 0.8727\n",
            "Epoch 13/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.5365 - acc: 0.8525 - val_loss: 0.5198 - val_acc: 0.8873\n",
            "Epoch 14/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.5114 - acc: 0.8643 - val_loss: 0.5204 - val_acc: 0.8545\n",
            "Epoch 15/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.4992 - acc: 0.8752 - val_loss: 0.5167 - val_acc: 0.8655\n",
            "Epoch 16/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.4906 - acc: 0.8716 - val_loss: 0.5518 - val_acc: 0.8255\n",
            "Epoch 17/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.4792 - acc: 0.8634 - val_loss: 0.4972 - val_acc: 0.8618\n",
            "Epoch 18/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.4647 - acc: 0.8761 - val_loss: 0.4950 - val_acc: 0.8727\n",
            "Epoch 19/500\n",
            "1098/1098 [==============================] - 0s 268us/sample - loss: 0.4455 - acc: 0.8752 - val_loss: 0.4872 - val_acc: 0.8655\n",
            "Epoch 20/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.4643 - acc: 0.8679 - val_loss: 0.4544 - val_acc: 0.9018\n",
            "Epoch 21/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.4434 - acc: 0.8743 - val_loss: 0.4478 - val_acc: 0.8982\n",
            "Epoch 22/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.4409 - acc: 0.8761 - val_loss: 0.5141 - val_acc: 0.8436\n",
            "Epoch 23/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.4230 - acc: 0.8816 - val_loss: 0.5376 - val_acc: 0.8509\n",
            "Epoch 24/500\n",
            "1098/1098 [==============================] - 0s 305us/sample - loss: 0.4350 - acc: 0.8789 - val_loss: 0.4268 - val_acc: 0.8945\n",
            "Epoch 25/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.3988 - acc: 0.8871 - val_loss: 0.4073 - val_acc: 0.9091\n",
            "Epoch 26/500\n",
            "1098/1098 [==============================] - 0s 301us/sample - loss: 0.4123 - acc: 0.8944 - val_loss: 0.4231 - val_acc: 0.8909\n",
            "Epoch 27/500\n",
            "1098/1098 [==============================] - 0s 295us/sample - loss: 0.4128 - acc: 0.8898 - val_loss: 0.4048 - val_acc: 0.9091\n",
            "Epoch 28/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.4171 - acc: 0.8789 - val_loss: 0.4196 - val_acc: 0.8836\n",
            "Epoch 29/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.4092 - acc: 0.8807 - val_loss: 0.4416 - val_acc: 0.8618\n",
            "Epoch 30/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.4059 - acc: 0.8816 - val_loss: 0.3977 - val_acc: 0.8873\n",
            "Epoch 31/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.3876 - acc: 0.8907 - val_loss: 0.4462 - val_acc: 0.8836\n",
            "Epoch 32/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.4007 - acc: 0.8862 - val_loss: 0.4092 - val_acc: 0.9018\n",
            "Epoch 33/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.4018 - acc: 0.8770 - val_loss: 0.4657 - val_acc: 0.8473\n",
            "Epoch 34/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.3995 - acc: 0.8898 - val_loss: 0.4386 - val_acc: 0.8655\n",
            "Epoch 35/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.3915 - acc: 0.8862 - val_loss: 0.3921 - val_acc: 0.8873\n",
            "Epoch 36/500\n",
            "1098/1098 [==============================] - 0s 308us/sample - loss: 0.3941 - acc: 0.8789 - val_loss: 0.4180 - val_acc: 0.8800\n",
            "Epoch 37/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.3695 - acc: 0.8971 - val_loss: 0.3684 - val_acc: 0.9200\n",
            "Epoch 38/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.3845 - acc: 0.8843 - val_loss: 0.4136 - val_acc: 0.8800\n",
            "Epoch 39/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.3672 - acc: 0.8934 - val_loss: 0.4016 - val_acc: 0.8727\n",
            "Epoch 40/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.3794 - acc: 0.8834 - val_loss: 0.3606 - val_acc: 0.9091\n",
            "Epoch 41/500\n",
            "1098/1098 [==============================] - 0s 304us/sample - loss: 0.3492 - acc: 0.9035 - val_loss: 0.3607 - val_acc: 0.9091\n",
            "Epoch 42/500\n",
            "1098/1098 [==============================] - 0s 267us/sample - loss: 0.3723 - acc: 0.8916 - val_loss: 0.3858 - val_acc: 0.8982\n",
            "Epoch 43/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.3601 - acc: 0.8962 - val_loss: 0.3776 - val_acc: 0.8945\n",
            "Epoch 44/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.3868 - acc: 0.8834 - val_loss: 0.3642 - val_acc: 0.9164\n",
            "Epoch 45/500\n",
            "1098/1098 [==============================] - 0s 294us/sample - loss: 0.3617 - acc: 0.8989 - val_loss: 0.3774 - val_acc: 0.8873\n",
            "Epoch 46/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.3538 - acc: 0.9035 - val_loss: 0.3482 - val_acc: 0.9200\n",
            "Epoch 47/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.3605 - acc: 0.8934 - val_loss: 0.4357 - val_acc: 0.8509\n",
            "Epoch 48/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.3640 - acc: 0.8980 - val_loss: 0.4253 - val_acc: 0.8800\n",
            "Epoch 49/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.3588 - acc: 0.8898 - val_loss: 0.3411 - val_acc: 0.9236\n",
            "Epoch 50/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.3585 - acc: 0.8907 - val_loss: 0.3759 - val_acc: 0.8982\n",
            "Epoch 51/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.3404 - acc: 0.8989 - val_loss: 0.3497 - val_acc: 0.9127\n",
            "Epoch 52/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.3363 - acc: 0.9044 - val_loss: 0.3594 - val_acc: 0.8945\n",
            "Epoch 53/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.3317 - acc: 0.8980 - val_loss: 0.3947 - val_acc: 0.8691\n",
            "Epoch 54/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.3487 - acc: 0.8916 - val_loss: 0.4316 - val_acc: 0.8582\n",
            "Epoch 55/500\n",
            "1098/1098 [==============================] - 0s 297us/sample - loss: 0.3516 - acc: 0.8934 - val_loss: 0.3502 - val_acc: 0.9018\n",
            "Epoch 56/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.3338 - acc: 0.9007 - val_loss: 0.3265 - val_acc: 0.9236\n",
            "Epoch 57/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.3267 - acc: 0.9080 - val_loss: 0.3671 - val_acc: 0.8800\n",
            "Epoch 58/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.3335 - acc: 0.8980 - val_loss: 0.3242 - val_acc: 0.9091\n",
            "Epoch 59/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.3471 - acc: 0.8816 - val_loss: 0.3605 - val_acc: 0.8982\n",
            "Epoch 60/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.3333 - acc: 0.8962 - val_loss: 0.4180 - val_acc: 0.8545\n",
            "Epoch 61/500\n",
            "1098/1098 [==============================] - 0s 308us/sample - loss: 0.3322 - acc: 0.8980 - val_loss: 0.3273 - val_acc: 0.9091\n",
            "Epoch 62/500\n",
            "1098/1098 [==============================] - 0s 303us/sample - loss: 0.3281 - acc: 0.9071 - val_loss: 0.3295 - val_acc: 0.9127\n",
            "Epoch 63/500\n",
            "1098/1098 [==============================] - 0s 304us/sample - loss: 0.3263 - acc: 0.8998 - val_loss: 0.3334 - val_acc: 0.9091\n",
            "Epoch 64/500\n",
            "1098/1098 [==============================] - 0s 298us/sample - loss: 0.3292 - acc: 0.8989 - val_loss: 0.3370 - val_acc: 0.9127\n",
            "Epoch 65/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.3288 - acc: 0.9053 - val_loss: 0.3552 - val_acc: 0.9018\n",
            "Epoch 66/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.3138 - acc: 0.9080 - val_loss: 0.3900 - val_acc: 0.8691\n",
            "Epoch 67/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.3179 - acc: 0.9016 - val_loss: 0.3283 - val_acc: 0.9273\n",
            "Epoch 68/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.3500 - acc: 0.8871 - val_loss: 0.3467 - val_acc: 0.9018\n",
            "Epoch 69/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.3065 - acc: 0.9089 - val_loss: 0.3411 - val_acc: 0.9055\n",
            "Epoch 70/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.3210 - acc: 0.8980 - val_loss: 0.3339 - val_acc: 0.9091\n",
            "Epoch 71/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.3185 - acc: 0.9007 - val_loss: 0.3205 - val_acc: 0.9200\n",
            "Epoch 72/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.3264 - acc: 0.8989 - val_loss: 0.3564 - val_acc: 0.8800\n",
            "Epoch 73/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.3314 - acc: 0.8980 - val_loss: 0.3365 - val_acc: 0.8945\n",
            "Epoch 74/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.3127 - acc: 0.9026 - val_loss: 0.3374 - val_acc: 0.9164\n",
            "Epoch 75/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.3169 - acc: 0.9026 - val_loss: 0.3156 - val_acc: 0.9164\n",
            "Epoch 76/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.3194 - acc: 0.8980 - val_loss: 0.4045 - val_acc: 0.8655\n",
            "Epoch 77/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.3226 - acc: 0.9026 - val_loss: 0.3470 - val_acc: 0.9018\n",
            "Epoch 78/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.3123 - acc: 0.9080 - val_loss: 0.3574 - val_acc: 0.8691\n",
            "Epoch 79/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.3171 - acc: 0.9016 - val_loss: 0.3198 - val_acc: 0.9091\n",
            "Epoch 80/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.3150 - acc: 0.9098 - val_loss: 0.3128 - val_acc: 0.9127\n",
            "Epoch 81/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.3070 - acc: 0.9089 - val_loss: 0.3089 - val_acc: 0.9200\n",
            "Epoch 82/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.3102 - acc: 0.8934 - val_loss: 0.3324 - val_acc: 0.8945\n",
            "Epoch 83/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.3085 - acc: 0.9080 - val_loss: 0.3248 - val_acc: 0.9055\n",
            "Epoch 84/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2929 - acc: 0.9107 - val_loss: 0.3784 - val_acc: 0.8655\n",
            "Epoch 85/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.3080 - acc: 0.9026 - val_loss: 0.3139 - val_acc: 0.9127\n",
            "Epoch 86/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.3138 - acc: 0.9126 - val_loss: 0.3224 - val_acc: 0.8982\n",
            "Epoch 87/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.3033 - acc: 0.9071 - val_loss: 0.4626 - val_acc: 0.8473\n",
            "Epoch 88/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.3295 - acc: 0.9016 - val_loss: 0.3087 - val_acc: 0.9164\n",
            "Epoch 89/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.3007 - acc: 0.9044 - val_loss: 0.3633 - val_acc: 0.8800\n",
            "Epoch 90/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.3108 - acc: 0.9026 - val_loss: 0.3119 - val_acc: 0.9055\n",
            "Epoch 91/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.3013 - acc: 0.9144 - val_loss: 0.3260 - val_acc: 0.8945\n",
            "Epoch 92/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2975 - acc: 0.9080 - val_loss: 0.3011 - val_acc: 0.9200\n",
            "Epoch 93/500\n",
            "1098/1098 [==============================] - 0s 303us/sample - loss: 0.2876 - acc: 0.9117 - val_loss: 0.3095 - val_acc: 0.9127\n",
            "Epoch 94/500\n",
            "1098/1098 [==============================] - 0s 300us/sample - loss: 0.3049 - acc: 0.9089 - val_loss: 0.2998 - val_acc: 0.9236\n",
            "Epoch 95/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.2958 - acc: 0.9153 - val_loss: 0.3333 - val_acc: 0.9018\n",
            "Epoch 96/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2836 - acc: 0.9107 - val_loss: 0.3151 - val_acc: 0.9055\n",
            "Epoch 97/500\n",
            "1098/1098 [==============================] - 0s 305us/sample - loss: 0.2942 - acc: 0.9089 - val_loss: 0.4092 - val_acc: 0.8582\n",
            "Epoch 98/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.3135 - acc: 0.8944 - val_loss: 0.3157 - val_acc: 0.9091\n",
            "Epoch 99/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2786 - acc: 0.9180 - val_loss: 0.2960 - val_acc: 0.9127\n",
            "Epoch 100/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2638 - acc: 0.9235 - val_loss: 0.2949 - val_acc: 0.9127\n",
            "Epoch 101/500\n",
            "1098/1098 [==============================] - 0s 295us/sample - loss: 0.2873 - acc: 0.9098 - val_loss: 0.3059 - val_acc: 0.9127\n",
            "Epoch 102/500\n",
            "1098/1098 [==============================] - 0s 295us/sample - loss: 0.2839 - acc: 0.9144 - val_loss: 0.3166 - val_acc: 0.9164\n",
            "Epoch 103/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2819 - acc: 0.9135 - val_loss: 0.3102 - val_acc: 0.9127\n",
            "Epoch 104/500\n",
            "1098/1098 [==============================] - 0s 303us/sample - loss: 0.2915 - acc: 0.9071 - val_loss: 0.3242 - val_acc: 0.9018\n",
            "Epoch 105/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2855 - acc: 0.9107 - val_loss: 0.2924 - val_acc: 0.9273\n",
            "Epoch 106/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2945 - acc: 0.9016 - val_loss: 0.2969 - val_acc: 0.9200\n",
            "Epoch 107/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.2941 - acc: 0.8971 - val_loss: 0.3088 - val_acc: 0.9127\n",
            "Epoch 108/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.3071 - acc: 0.8980 - val_loss: 0.3031 - val_acc: 0.9055\n",
            "Epoch 109/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2818 - acc: 0.9144 - val_loss: 0.3124 - val_acc: 0.8982\n",
            "Epoch 110/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2812 - acc: 0.9107 - val_loss: 0.3031 - val_acc: 0.9127\n",
            "Epoch 111/500\n",
            "1098/1098 [==============================] - 0s 305us/sample - loss: 0.2829 - acc: 0.9053 - val_loss: 0.3601 - val_acc: 0.9091\n",
            "Epoch 112/500\n",
            "1098/1098 [==============================] - 0s 294us/sample - loss: 0.2861 - acc: 0.9062 - val_loss: 0.3049 - val_acc: 0.8982\n",
            "Epoch 113/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2813 - acc: 0.9199 - val_loss: 0.2947 - val_acc: 0.9127\n",
            "Epoch 114/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2858 - acc: 0.9098 - val_loss: 0.3398 - val_acc: 0.8800\n",
            "Epoch 115/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2951 - acc: 0.9117 - val_loss: 0.3470 - val_acc: 0.8727\n",
            "Epoch 116/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2846 - acc: 0.9080 - val_loss: 0.2870 - val_acc: 0.9200\n",
            "Epoch 117/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2748 - acc: 0.9044 - val_loss: 0.2884 - val_acc: 0.9200\n",
            "Epoch 118/500\n",
            "1098/1098 [==============================] - 0s 264us/sample - loss: 0.2794 - acc: 0.9107 - val_loss: 0.2945 - val_acc: 0.9273\n",
            "Epoch 119/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2792 - acc: 0.9126 - val_loss: 0.3166 - val_acc: 0.8982\n",
            "Epoch 120/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2897 - acc: 0.9098 - val_loss: 0.3148 - val_acc: 0.9055\n",
            "Epoch 121/500\n",
            "1098/1098 [==============================] - 0s 263us/sample - loss: 0.2768 - acc: 0.9016 - val_loss: 0.2898 - val_acc: 0.9164\n",
            "Epoch 122/500\n",
            "1098/1098 [==============================] - 0s 300us/sample - loss: 0.2699 - acc: 0.9180 - val_loss: 0.3073 - val_acc: 0.9127\n",
            "Epoch 123/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2773 - acc: 0.9153 - val_loss: 0.3204 - val_acc: 0.9091\n",
            "Epoch 124/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2914 - acc: 0.9035 - val_loss: 0.3152 - val_acc: 0.9091\n",
            "Epoch 125/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2856 - acc: 0.9062 - val_loss: 0.2824 - val_acc: 0.9309\n",
            "Epoch 126/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2719 - acc: 0.9162 - val_loss: 0.2960 - val_acc: 0.9127\n",
            "Epoch 127/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2785 - acc: 0.9117 - val_loss: 0.2889 - val_acc: 0.9236\n",
            "Epoch 128/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2839 - acc: 0.9098 - val_loss: 0.2938 - val_acc: 0.9200\n",
            "Epoch 129/500\n",
            "1098/1098 [==============================] - 0s 267us/sample - loss: 0.2830 - acc: 0.9107 - val_loss: 0.3159 - val_acc: 0.8982\n",
            "Epoch 130/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2730 - acc: 0.9135 - val_loss: 0.2766 - val_acc: 0.9273\n",
            "Epoch 131/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2731 - acc: 0.9098 - val_loss: 0.2920 - val_acc: 0.9164\n",
            "Epoch 132/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2747 - acc: 0.9180 - val_loss: 0.3328 - val_acc: 0.8873\n",
            "Epoch 133/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2863 - acc: 0.9089 - val_loss: 0.2893 - val_acc: 0.9127\n",
            "Epoch 134/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.2681 - acc: 0.9162 - val_loss: 0.3005 - val_acc: 0.9055\n",
            "Epoch 135/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2676 - acc: 0.9226 - val_loss: 0.2965 - val_acc: 0.9091\n",
            "Epoch 136/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2574 - acc: 0.9126 - val_loss: 0.2934 - val_acc: 0.9309\n",
            "Epoch 137/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2644 - acc: 0.9171 - val_loss: 0.3081 - val_acc: 0.9164\n",
            "Epoch 138/500\n",
            "1098/1098 [==============================] - 0s 270us/sample - loss: 0.2700 - acc: 0.9171 - val_loss: 0.3097 - val_acc: 0.9127\n",
            "Epoch 139/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2674 - acc: 0.9107 - val_loss: 0.2823 - val_acc: 0.9091\n",
            "Epoch 140/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2743 - acc: 0.9089 - val_loss: 0.2952 - val_acc: 0.9273\n",
            "Epoch 141/500\n",
            "1098/1098 [==============================] - 0s 269us/sample - loss: 0.2622 - acc: 0.9171 - val_loss: 0.2914 - val_acc: 0.9236\n",
            "Epoch 142/500\n",
            "1098/1098 [==============================] - 0s 268us/sample - loss: 0.2676 - acc: 0.9144 - val_loss: 0.3642 - val_acc: 0.8691\n",
            "Epoch 143/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2646 - acc: 0.9162 - val_loss: 0.2927 - val_acc: 0.9127\n",
            "Epoch 144/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2604 - acc: 0.9144 - val_loss: 0.2862 - val_acc: 0.9127\n",
            "Epoch 145/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2733 - acc: 0.9171 - val_loss: 0.2959 - val_acc: 0.9018\n",
            "Epoch 146/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2682 - acc: 0.9171 - val_loss: 0.2970 - val_acc: 0.9018\n",
            "Epoch 147/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2718 - acc: 0.9126 - val_loss: 0.3010 - val_acc: 0.9091\n",
            "Epoch 148/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2578 - acc: 0.9171 - val_loss: 0.3202 - val_acc: 0.9018\n",
            "Epoch 149/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2652 - acc: 0.9144 - val_loss: 0.2896 - val_acc: 0.9127\n",
            "Epoch 150/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2824 - acc: 0.9080 - val_loss: 0.2897 - val_acc: 0.9091\n",
            "Epoch 151/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2638 - acc: 0.9153 - val_loss: 0.2797 - val_acc: 0.9127\n",
            "Epoch 152/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.2654 - acc: 0.9089 - val_loss: 0.2795 - val_acc: 0.9055\n",
            "Epoch 153/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2511 - acc: 0.9235 - val_loss: 0.2743 - val_acc: 0.9200\n",
            "Epoch 154/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2572 - acc: 0.9171 - val_loss: 0.2828 - val_acc: 0.9127\n",
            "Epoch 155/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2599 - acc: 0.9208 - val_loss: 0.3116 - val_acc: 0.9018\n",
            "Epoch 156/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2611 - acc: 0.9098 - val_loss: 0.2726 - val_acc: 0.9236\n",
            "Epoch 157/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2608 - acc: 0.9089 - val_loss: 0.2781 - val_acc: 0.9164\n",
            "Epoch 158/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2631 - acc: 0.9226 - val_loss: 0.2731 - val_acc: 0.9236\n",
            "Epoch 159/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2597 - acc: 0.9162 - val_loss: 0.2911 - val_acc: 0.9127\n",
            "Epoch 160/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2702 - acc: 0.9044 - val_loss: 0.2767 - val_acc: 0.9200\n",
            "Epoch 161/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2633 - acc: 0.9098 - val_loss: 0.2677 - val_acc: 0.9164\n",
            "Epoch 162/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2724 - acc: 0.9117 - val_loss: 0.2827 - val_acc: 0.9200\n",
            "Epoch 163/500\n",
            "1098/1098 [==============================] - 0s 295us/sample - loss: 0.2580 - acc: 0.9144 - val_loss: 0.2752 - val_acc: 0.9200\n",
            "Epoch 164/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2545 - acc: 0.9180 - val_loss: 0.2702 - val_acc: 0.9200\n",
            "Epoch 165/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2471 - acc: 0.9208 - val_loss: 0.2760 - val_acc: 0.9200\n",
            "Epoch 166/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2432 - acc: 0.9180 - val_loss: 0.2714 - val_acc: 0.9200\n",
            "Epoch 167/500\n",
            "1098/1098 [==============================] - 0s 296us/sample - loss: 0.2714 - acc: 0.9098 - val_loss: 0.2805 - val_acc: 0.9200\n",
            "Epoch 168/500\n",
            "1098/1098 [==============================] - 0s 309us/sample - loss: 0.2575 - acc: 0.9126 - val_loss: 0.2730 - val_acc: 0.9200\n",
            "Epoch 169/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2498 - acc: 0.9153 - val_loss: 0.2939 - val_acc: 0.9091\n",
            "Epoch 170/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2599 - acc: 0.9135 - val_loss: 0.2711 - val_acc: 0.9127\n",
            "Epoch 171/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2467 - acc: 0.9253 - val_loss: 0.2813 - val_acc: 0.9091\n",
            "Epoch 172/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2545 - acc: 0.9162 - val_loss: 0.3119 - val_acc: 0.8982\n",
            "Epoch 173/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2744 - acc: 0.9062 - val_loss: 0.2878 - val_acc: 0.9091\n",
            "Epoch 174/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2425 - acc: 0.9244 - val_loss: 0.2819 - val_acc: 0.9164\n",
            "Epoch 175/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2684 - acc: 0.9135 - val_loss: 0.2924 - val_acc: 0.9055\n",
            "Epoch 176/500\n",
            "1098/1098 [==============================] - 0s 295us/sample - loss: 0.2666 - acc: 0.9135 - val_loss: 0.2801 - val_acc: 0.9164\n",
            "Epoch 177/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2521 - acc: 0.9226 - val_loss: 0.2732 - val_acc: 0.9164\n",
            "Epoch 178/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2779 - acc: 0.9035 - val_loss: 0.3296 - val_acc: 0.8982\n",
            "Epoch 179/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2735 - acc: 0.9035 - val_loss: 0.3322 - val_acc: 0.8982\n",
            "Epoch 180/500\n",
            "1098/1098 [==============================] - 0s 302us/sample - loss: 0.2708 - acc: 0.9089 - val_loss: 0.3062 - val_acc: 0.9018\n",
            "Epoch 181/500\n",
            "1098/1098 [==============================] - 0s 301us/sample - loss: 0.2576 - acc: 0.9162 - val_loss: 0.2803 - val_acc: 0.9055\n",
            "Epoch 182/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2676 - acc: 0.9126 - val_loss: 0.2896 - val_acc: 0.9018\n",
            "Epoch 183/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2557 - acc: 0.9162 - val_loss: 0.2776 - val_acc: 0.9055\n",
            "Epoch 184/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2428 - acc: 0.9226 - val_loss: 0.2855 - val_acc: 0.9091\n",
            "Epoch 185/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2445 - acc: 0.9226 - val_loss: 0.3007 - val_acc: 0.9018\n",
            "Epoch 186/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2510 - acc: 0.9217 - val_loss: 0.3024 - val_acc: 0.9055\n",
            "Epoch 187/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2479 - acc: 0.9253 - val_loss: 0.2887 - val_acc: 0.9127\n",
            "Epoch 188/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2564 - acc: 0.9144 - val_loss: 0.2989 - val_acc: 0.9055\n",
            "Epoch 189/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2546 - acc: 0.9153 - val_loss: 0.3221 - val_acc: 0.9055\n",
            "Epoch 190/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2677 - acc: 0.9107 - val_loss: 0.2899 - val_acc: 0.9018\n",
            "Epoch 191/500\n",
            "1098/1098 [==============================] - 0s 295us/sample - loss: 0.2567 - acc: 0.9144 - val_loss: 0.2844 - val_acc: 0.9164\n",
            "Epoch 192/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2443 - acc: 0.9271 - val_loss: 0.2886 - val_acc: 0.9127\n",
            "Epoch 193/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2513 - acc: 0.9217 - val_loss: 0.2738 - val_acc: 0.9345\n",
            "Epoch 194/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2412 - acc: 0.9226 - val_loss: 0.2961 - val_acc: 0.9018\n",
            "Epoch 195/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2492 - acc: 0.9153 - val_loss: 0.2834 - val_acc: 0.9127\n",
            "Epoch 196/500\n",
            "1098/1098 [==============================] - 0s 302us/sample - loss: 0.2458 - acc: 0.9189 - val_loss: 0.2926 - val_acc: 0.9055\n",
            "Epoch 197/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.2366 - acc: 0.9235 - val_loss: 0.2805 - val_acc: 0.9164\n",
            "Epoch 198/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2438 - acc: 0.9217 - val_loss: 0.2894 - val_acc: 0.9200\n",
            "Epoch 199/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2483 - acc: 0.9180 - val_loss: 0.2930 - val_acc: 0.9018\n",
            "Epoch 200/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.2587 - acc: 0.9144 - val_loss: 0.2818 - val_acc: 0.9164\n",
            "Epoch 201/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2586 - acc: 0.9180 - val_loss: 0.2785 - val_acc: 0.9127\n",
            "Epoch 202/500\n",
            "1098/1098 [==============================] - 0s 269us/sample - loss: 0.2504 - acc: 0.9126 - val_loss: 0.2759 - val_acc: 0.9127\n",
            "Epoch 203/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2389 - acc: 0.9271 - val_loss: 0.2947 - val_acc: 0.9091\n",
            "Epoch 204/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2477 - acc: 0.9144 - val_loss: 0.3025 - val_acc: 0.9164\n",
            "Epoch 205/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2539 - acc: 0.9162 - val_loss: 0.3477 - val_acc: 0.8836\n",
            "Epoch 206/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2598 - acc: 0.9117 - val_loss: 0.3859 - val_acc: 0.8618\n",
            "Epoch 207/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2816 - acc: 0.9071 - val_loss: 0.3077 - val_acc: 0.9018\n",
            "Epoch 208/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2551 - acc: 0.9189 - val_loss: 0.2832 - val_acc: 0.9309\n",
            "Epoch 209/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.2548 - acc: 0.9080 - val_loss: 0.2712 - val_acc: 0.9200\n",
            "Epoch 210/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2474 - acc: 0.9162 - val_loss: 0.3246 - val_acc: 0.9055\n",
            "Epoch 211/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2540 - acc: 0.9144 - val_loss: 0.3188 - val_acc: 0.8982\n",
            "Epoch 212/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2526 - acc: 0.9135 - val_loss: 0.2911 - val_acc: 0.9236\n",
            "Epoch 213/500\n",
            "1098/1098 [==============================] - 0s 261us/sample - loss: 0.2423 - acc: 0.9208 - val_loss: 0.2656 - val_acc: 0.9273\n",
            "Epoch 214/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2344 - acc: 0.9244 - val_loss: 0.2693 - val_acc: 0.9164\n",
            "Epoch 215/500\n",
            "1098/1098 [==============================] - 0s 270us/sample - loss: 0.2496 - acc: 0.9171 - val_loss: 0.2924 - val_acc: 0.9055\n",
            "Epoch 216/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.2389 - acc: 0.9262 - val_loss: 0.2732 - val_acc: 0.9200\n",
            "Epoch 217/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2392 - acc: 0.9253 - val_loss: 0.2721 - val_acc: 0.9164\n",
            "Epoch 218/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2554 - acc: 0.9171 - val_loss: 0.2711 - val_acc: 0.9200\n",
            "Epoch 219/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2353 - acc: 0.9235 - val_loss: 0.2745 - val_acc: 0.9091\n",
            "Epoch 220/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2267 - acc: 0.9226 - val_loss: 0.2876 - val_acc: 0.9127\n",
            "Epoch 221/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2476 - acc: 0.9171 - val_loss: 0.2693 - val_acc: 0.9164\n",
            "Epoch 222/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2463 - acc: 0.9135 - val_loss: 0.2762 - val_acc: 0.9018\n",
            "Epoch 223/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2350 - acc: 0.9235 - val_loss: 0.2722 - val_acc: 0.9127\n",
            "Epoch 224/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2408 - acc: 0.9189 - val_loss: 0.2700 - val_acc: 0.9164\n",
            "Epoch 225/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2387 - acc: 0.9244 - val_loss: 0.2766 - val_acc: 0.9091\n",
            "Epoch 226/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2293 - acc: 0.9253 - val_loss: 0.2873 - val_acc: 0.8982\n",
            "Epoch 227/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2333 - acc: 0.9226 - val_loss: 0.2785 - val_acc: 0.9127\n",
            "Epoch 228/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2326 - acc: 0.9262 - val_loss: 0.3335 - val_acc: 0.8836\n",
            "Epoch 229/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2502 - acc: 0.9208 - val_loss: 0.2893 - val_acc: 0.9018\n",
            "Epoch 230/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2349 - acc: 0.9226 - val_loss: 0.2737 - val_acc: 0.9236\n",
            "Epoch 231/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2340 - acc: 0.9244 - val_loss: 0.2829 - val_acc: 0.9164\n",
            "Epoch 232/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.2331 - acc: 0.9199 - val_loss: 0.2838 - val_acc: 0.9127\n",
            "Epoch 233/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2525 - acc: 0.9144 - val_loss: 0.3257 - val_acc: 0.9055\n",
            "Epoch 234/500\n",
            "1098/1098 [==============================] - 0s 307us/sample - loss: 0.2584 - acc: 0.9162 - val_loss: 0.2770 - val_acc: 0.9200\n",
            "Epoch 235/500\n",
            "1098/1098 [==============================] - 0s 294us/sample - loss: 0.2366 - acc: 0.9217 - val_loss: 0.2778 - val_acc: 0.9127\n",
            "Epoch 236/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2398 - acc: 0.9199 - val_loss: 0.2901 - val_acc: 0.9091\n",
            "Epoch 237/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2436 - acc: 0.9235 - val_loss: 0.2764 - val_acc: 0.9236\n",
            "Epoch 238/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2284 - acc: 0.9317 - val_loss: 0.2710 - val_acc: 0.9127\n",
            "Epoch 239/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2392 - acc: 0.9189 - val_loss: 0.2975 - val_acc: 0.9164\n",
            "Epoch 240/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2349 - acc: 0.9271 - val_loss: 0.2785 - val_acc: 0.9273\n",
            "Epoch 241/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2368 - acc: 0.9180 - val_loss: 0.2724 - val_acc: 0.9164\n",
            "Epoch 242/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2449 - acc: 0.9162 - val_loss: 0.3027 - val_acc: 0.8945\n",
            "Epoch 243/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2541 - acc: 0.9189 - val_loss: 0.2724 - val_acc: 0.9164\n",
            "Epoch 244/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2339 - acc: 0.9262 - val_loss: 0.2900 - val_acc: 0.9236\n",
            "Epoch 245/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2273 - acc: 0.9271 - val_loss: 0.2935 - val_acc: 0.9236\n",
            "Epoch 246/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2287 - acc: 0.9217 - val_loss: 0.2825 - val_acc: 0.9091\n",
            "Epoch 247/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2261 - acc: 0.9217 - val_loss: 0.2978 - val_acc: 0.9091\n",
            "Epoch 248/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2405 - acc: 0.9217 - val_loss: 0.2715 - val_acc: 0.9273\n",
            "Epoch 249/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2311 - acc: 0.9226 - val_loss: 0.2724 - val_acc: 0.9382\n",
            "Epoch 250/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2415 - acc: 0.9199 - val_loss: 0.2687 - val_acc: 0.9309\n",
            "Epoch 251/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2225 - acc: 0.9299 - val_loss: 0.2696 - val_acc: 0.9345\n",
            "Epoch 252/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2342 - acc: 0.9226 - val_loss: 0.2686 - val_acc: 0.9345\n",
            "Epoch 253/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2346 - acc: 0.9326 - val_loss: 0.2870 - val_acc: 0.9164\n",
            "Epoch 254/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2302 - acc: 0.9244 - val_loss: 0.2759 - val_acc: 0.9200\n",
            "Epoch 255/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2332 - acc: 0.9226 - val_loss: 0.3496 - val_acc: 0.8945\n",
            "Epoch 256/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2535 - acc: 0.9199 - val_loss: 0.2780 - val_acc: 0.9200\n",
            "Epoch 257/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2352 - acc: 0.9208 - val_loss: 0.2750 - val_acc: 0.9164\n",
            "Epoch 258/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2335 - acc: 0.9180 - val_loss: 0.2687 - val_acc: 0.9200\n",
            "Epoch 259/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2322 - acc: 0.9262 - val_loss: 0.2775 - val_acc: 0.9164\n",
            "Epoch 260/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2239 - acc: 0.9262 - val_loss: 0.2740 - val_acc: 0.9309\n",
            "Epoch 261/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2278 - acc: 0.9208 - val_loss: 0.2821 - val_acc: 0.9273\n",
            "Epoch 262/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2220 - acc: 0.9281 - val_loss: 0.2801 - val_acc: 0.9200\n",
            "Epoch 263/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2378 - acc: 0.9271 - val_loss: 0.2729 - val_acc: 0.9236\n",
            "Epoch 264/500\n",
            "1098/1098 [==============================] - 0s 268us/sample - loss: 0.2324 - acc: 0.9253 - val_loss: 0.2759 - val_acc: 0.9309\n",
            "Epoch 265/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2289 - acc: 0.9180 - val_loss: 0.2681 - val_acc: 0.9273\n",
            "Epoch 266/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2243 - acc: 0.9290 - val_loss: 0.2757 - val_acc: 0.9200\n",
            "Epoch 267/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.2249 - acc: 0.9299 - val_loss: 0.2975 - val_acc: 0.9018\n",
            "Epoch 268/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.2191 - acc: 0.9281 - val_loss: 0.2789 - val_acc: 0.9273\n",
            "Epoch 269/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2256 - acc: 0.9244 - val_loss: 0.2919 - val_acc: 0.9164\n",
            "Epoch 270/500\n",
            "1098/1098 [==============================] - 0s 297us/sample - loss: 0.2251 - acc: 0.9262 - val_loss: 0.2676 - val_acc: 0.9345\n",
            "Epoch 271/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2294 - acc: 0.9180 - val_loss: 0.2945 - val_acc: 0.9127\n",
            "Epoch 272/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2311 - acc: 0.9208 - val_loss: 0.2826 - val_acc: 0.9018\n",
            "Epoch 273/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2536 - acc: 0.9117 - val_loss: 0.3071 - val_acc: 0.9236\n",
            "Epoch 274/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2331 - acc: 0.9199 - val_loss: 0.3060 - val_acc: 0.8945\n",
            "Epoch 275/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2320 - acc: 0.9217 - val_loss: 0.2957 - val_acc: 0.9091\n",
            "Epoch 276/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2423 - acc: 0.9180 - val_loss: 0.2824 - val_acc: 0.9236\n",
            "Epoch 277/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2288 - acc: 0.9226 - val_loss: 0.2787 - val_acc: 0.9164\n",
            "Epoch 278/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2231 - acc: 0.9262 - val_loss: 0.2814 - val_acc: 0.9382\n",
            "Epoch 279/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2127 - acc: 0.9372 - val_loss: 0.2917 - val_acc: 0.9127\n",
            "Epoch 280/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2233 - acc: 0.9253 - val_loss: 0.3008 - val_acc: 0.9164\n",
            "Epoch 281/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2250 - acc: 0.9217 - val_loss: 0.2939 - val_acc: 0.9055\n",
            "Epoch 282/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2290 - acc: 0.9162 - val_loss: 0.2989 - val_acc: 0.9055\n",
            "Epoch 283/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2302 - acc: 0.9262 - val_loss: 0.3522 - val_acc: 0.8945\n",
            "Epoch 284/500\n",
            "1098/1098 [==============================] - 0s 303us/sample - loss: 0.2306 - acc: 0.9244 - val_loss: 0.2885 - val_acc: 0.9164\n",
            "Epoch 285/500\n",
            "1098/1098 [==============================] - 0s 298us/sample - loss: 0.2328 - acc: 0.9281 - val_loss: 0.2872 - val_acc: 0.9382\n",
            "Epoch 286/500\n",
            "1098/1098 [==============================] - 0s 303us/sample - loss: 0.2261 - acc: 0.9253 - val_loss: 0.2799 - val_acc: 0.9127\n",
            "Epoch 287/500\n",
            "1098/1098 [==============================] - 0s 297us/sample - loss: 0.2266 - acc: 0.9189 - val_loss: 0.2785 - val_acc: 0.9236\n",
            "Epoch 288/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2162 - acc: 0.9299 - val_loss: 0.2750 - val_acc: 0.9200\n",
            "Epoch 289/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2254 - acc: 0.9253 - val_loss: 0.2857 - val_acc: 0.9309\n",
            "Epoch 290/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.2167 - acc: 0.9281 - val_loss: 0.2792 - val_acc: 0.9164\n",
            "Epoch 291/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2291 - acc: 0.9235 - val_loss: 0.2734 - val_acc: 0.9273\n",
            "Epoch 292/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2311 - acc: 0.9235 - val_loss: 0.3090 - val_acc: 0.9018\n",
            "Epoch 293/500\n",
            "1098/1098 [==============================] - 0s 267us/sample - loss: 0.2325 - acc: 0.9235 - val_loss: 0.2972 - val_acc: 0.9127\n",
            "Epoch 294/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2282 - acc: 0.9171 - val_loss: 0.2792 - val_acc: 0.9164\n",
            "Epoch 295/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2338 - acc: 0.9171 - val_loss: 0.2740 - val_acc: 0.9200\n",
            "Epoch 296/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2148 - acc: 0.9362 - val_loss: 0.2757 - val_acc: 0.9200\n",
            "Epoch 297/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2233 - acc: 0.9290 - val_loss: 0.2949 - val_acc: 0.8982\n",
            "Epoch 298/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2233 - acc: 0.9271 - val_loss: 0.2739 - val_acc: 0.9273\n",
            "Epoch 299/500\n",
            "1098/1098 [==============================] - 0s 261us/sample - loss: 0.2175 - acc: 0.9290 - val_loss: 0.2761 - val_acc: 0.9127\n",
            "Epoch 300/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2156 - acc: 0.9262 - val_loss: 0.3135 - val_acc: 0.9127\n",
            "Epoch 301/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2116 - acc: 0.9344 - val_loss: 0.3005 - val_acc: 0.9164\n",
            "Epoch 302/500\n",
            "1098/1098 [==============================] - 0s 265us/sample - loss: 0.2300 - acc: 0.9244 - val_loss: 0.2781 - val_acc: 0.9127\n",
            "Epoch 303/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2346 - acc: 0.9244 - val_loss: 0.3209 - val_acc: 0.8982\n",
            "Epoch 304/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2287 - acc: 0.9226 - val_loss: 0.3124 - val_acc: 0.9091\n",
            "Epoch 305/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2334 - acc: 0.9171 - val_loss: 0.2773 - val_acc: 0.9164\n",
            "Epoch 306/500\n",
            "1098/1098 [==============================] - 0s 296us/sample - loss: 0.2164 - acc: 0.9317 - val_loss: 0.2963 - val_acc: 0.9273\n",
            "Epoch 307/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2222 - acc: 0.9217 - val_loss: 0.2723 - val_acc: 0.9273\n",
            "Epoch 308/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2203 - acc: 0.9281 - val_loss: 0.3171 - val_acc: 0.9091\n",
            "Epoch 309/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.2277 - acc: 0.9235 - val_loss: 0.2829 - val_acc: 0.9091\n",
            "Epoch 310/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2342 - acc: 0.9189 - val_loss: 0.3073 - val_acc: 0.9236\n",
            "Epoch 311/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.2370 - acc: 0.9162 - val_loss: 0.2790 - val_acc: 0.9091\n",
            "Epoch 312/500\n",
            "1098/1098 [==============================] - 0s 294us/sample - loss: 0.2216 - acc: 0.9262 - val_loss: 0.2827 - val_acc: 0.9273\n",
            "Epoch 313/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2215 - acc: 0.9281 - val_loss: 0.3306 - val_acc: 0.9127\n",
            "Epoch 314/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2263 - acc: 0.9281 - val_loss: 0.2931 - val_acc: 0.8945\n",
            "Epoch 315/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2326 - acc: 0.9262 - val_loss: 0.2936 - val_acc: 0.9236\n",
            "Epoch 316/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2312 - acc: 0.9244 - val_loss: 0.3173 - val_acc: 0.9055\n",
            "Epoch 317/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2106 - acc: 0.9372 - val_loss: 0.3019 - val_acc: 0.9236\n",
            "Epoch 318/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2268 - acc: 0.9317 - val_loss: 0.3278 - val_acc: 0.9091\n",
            "Epoch 319/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2230 - acc: 0.9235 - val_loss: 0.2822 - val_acc: 0.9055\n",
            "Epoch 320/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2250 - acc: 0.9281 - val_loss: 0.2865 - val_acc: 0.9127\n",
            "Epoch 321/500\n",
            "1098/1098 [==============================] - 0s 310us/sample - loss: 0.2144 - acc: 0.9290 - val_loss: 0.3025 - val_acc: 0.9091\n",
            "Epoch 322/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.2210 - acc: 0.9271 - val_loss: 0.2918 - val_acc: 0.9164\n",
            "Epoch 323/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2300 - acc: 0.9208 - val_loss: 0.2960 - val_acc: 0.9309\n",
            "Epoch 324/500\n",
            "1098/1098 [==============================] - 0s 264us/sample - loss: 0.2077 - acc: 0.9326 - val_loss: 0.2884 - val_acc: 0.9127\n",
            "Epoch 325/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2261 - acc: 0.9281 - val_loss: 0.3000 - val_acc: 0.9164\n",
            "Epoch 326/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2212 - acc: 0.9299 - val_loss: 0.3071 - val_acc: 0.9091\n",
            "Epoch 327/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.2101 - acc: 0.9299 - val_loss: 0.2785 - val_acc: 0.9091\n",
            "Epoch 328/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2262 - acc: 0.9290 - val_loss: 0.2733 - val_acc: 0.9164\n",
            "Epoch 329/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2220 - acc: 0.9317 - val_loss: 0.2799 - val_acc: 0.9236\n",
            "Epoch 330/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2200 - acc: 0.9235 - val_loss: 0.2835 - val_acc: 0.9164\n",
            "Epoch 331/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2155 - acc: 0.9344 - val_loss: 0.2742 - val_acc: 0.9091\n",
            "Epoch 332/500\n",
            "1098/1098 [==============================] - 0s 302us/sample - loss: 0.2271 - acc: 0.9262 - val_loss: 0.2739 - val_acc: 0.9309\n",
            "Epoch 333/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2176 - acc: 0.9335 - val_loss: 0.2830 - val_acc: 0.9273\n",
            "Epoch 334/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2054 - acc: 0.9317 - val_loss: 0.2873 - val_acc: 0.9309\n",
            "Epoch 335/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2206 - acc: 0.9290 - val_loss: 0.2757 - val_acc: 0.9345\n",
            "Epoch 336/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2164 - acc: 0.9281 - val_loss: 0.2734 - val_acc: 0.9236\n",
            "Epoch 337/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.2066 - acc: 0.9317 - val_loss: 0.2905 - val_acc: 0.9236\n",
            "Epoch 338/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2124 - acc: 0.9326 - val_loss: 0.2691 - val_acc: 0.9382\n",
            "Epoch 339/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2087 - acc: 0.9308 - val_loss: 0.2793 - val_acc: 0.9127\n",
            "Epoch 340/500\n",
            "1098/1098 [==============================] - 0s 301us/sample - loss: 0.2126 - acc: 0.9362 - val_loss: 0.2792 - val_acc: 0.9382\n",
            "Epoch 341/500\n",
            "1098/1098 [==============================] - 0s 301us/sample - loss: 0.2256 - acc: 0.9217 - val_loss: 0.3182 - val_acc: 0.9273\n",
            "Epoch 342/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2055 - acc: 0.9353 - val_loss: 0.2910 - val_acc: 0.9236\n",
            "Epoch 343/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2028 - acc: 0.9353 - val_loss: 0.2827 - val_acc: 0.9164\n",
            "Epoch 344/500\n",
            "1098/1098 [==============================] - 0s 302us/sample - loss: 0.2159 - acc: 0.9317 - val_loss: 0.3008 - val_acc: 0.9236\n",
            "Epoch 345/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2109 - acc: 0.9317 - val_loss: 0.3206 - val_acc: 0.9055\n",
            "Epoch 346/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2319 - acc: 0.9208 - val_loss: 0.2894 - val_acc: 0.9164\n",
            "Epoch 347/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2329 - acc: 0.9235 - val_loss: 0.3074 - val_acc: 0.9055\n",
            "Epoch 348/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2207 - acc: 0.9253 - val_loss: 0.3201 - val_acc: 0.9018\n",
            "Epoch 349/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2230 - acc: 0.9180 - val_loss: 0.2981 - val_acc: 0.9018\n",
            "Epoch 350/500\n",
            "1098/1098 [==============================] - 0s 316us/sample - loss: 0.2182 - acc: 0.9262 - val_loss: 0.2908 - val_acc: 0.9309\n",
            "Epoch 351/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2227 - acc: 0.9281 - val_loss: 0.2907 - val_acc: 0.9345\n",
            "Epoch 352/500\n",
            "1098/1098 [==============================] - 0s 268us/sample - loss: 0.2386 - acc: 0.9171 - val_loss: 0.3150 - val_acc: 0.9127\n",
            "Epoch 353/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2159 - acc: 0.9353 - val_loss: 0.2931 - val_acc: 0.9200\n",
            "Epoch 354/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2212 - acc: 0.9217 - val_loss: 0.2755 - val_acc: 0.9273\n",
            "Epoch 355/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2101 - acc: 0.9381 - val_loss: 0.2742 - val_acc: 0.9236\n",
            "Epoch 356/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2065 - acc: 0.9326 - val_loss: 0.2738 - val_acc: 0.9455\n",
            "Epoch 357/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2186 - acc: 0.9244 - val_loss: 0.3051 - val_acc: 0.9127\n",
            "Epoch 358/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2322 - acc: 0.9235 - val_loss: 0.2853 - val_acc: 0.9273\n",
            "Epoch 359/500\n",
            "1098/1098 [==============================] - 0s 267us/sample - loss: 0.2126 - acc: 0.9326 - val_loss: 0.3144 - val_acc: 0.9018\n",
            "Epoch 360/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2318 - acc: 0.9199 - val_loss: 0.3096 - val_acc: 0.9018\n",
            "Epoch 361/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2199 - acc: 0.9299 - val_loss: 0.2716 - val_acc: 0.9418\n",
            "Epoch 362/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2077 - acc: 0.9235 - val_loss: 0.2788 - val_acc: 0.9345\n",
            "Epoch 363/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2118 - acc: 0.9317 - val_loss: 0.3254 - val_acc: 0.9091\n",
            "Epoch 364/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.2326 - acc: 0.9208 - val_loss: 0.2863 - val_acc: 0.9236\n",
            "Epoch 365/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2180 - acc: 0.9299 - val_loss: 0.2812 - val_acc: 0.9382\n",
            "Epoch 366/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2072 - acc: 0.9290 - val_loss: 0.2917 - val_acc: 0.9309\n",
            "Epoch 367/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2165 - acc: 0.9299 - val_loss: 0.2862 - val_acc: 0.9164\n",
            "Epoch 368/500\n",
            "1098/1098 [==============================] - 0s 269us/sample - loss: 0.2104 - acc: 0.9317 - val_loss: 0.2762 - val_acc: 0.9382\n",
            "Epoch 369/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2044 - acc: 0.9326 - val_loss: 0.2927 - val_acc: 0.9309\n",
            "Epoch 370/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2079 - acc: 0.9299 - val_loss: 0.3082 - val_acc: 0.9055\n",
            "Epoch 371/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2017 - acc: 0.9317 - val_loss: 0.2846 - val_acc: 0.9309\n",
            "Epoch 372/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.2059 - acc: 0.9335 - val_loss: 0.2878 - val_acc: 0.9200\n",
            "Epoch 373/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2206 - acc: 0.9262 - val_loss: 0.2985 - val_acc: 0.9127\n",
            "Epoch 374/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2109 - acc: 0.9353 - val_loss: 0.2760 - val_acc: 0.9200\n",
            "Epoch 375/500\n",
            "1098/1098 [==============================] - 0s 298us/sample - loss: 0.2081 - acc: 0.9262 - val_loss: 0.3025 - val_acc: 0.9200\n",
            "Epoch 376/500\n",
            "1098/1098 [==============================] - 0s 298us/sample - loss: 0.2161 - acc: 0.9317 - val_loss: 0.2913 - val_acc: 0.9309\n",
            "Epoch 377/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2056 - acc: 0.9281 - val_loss: 0.2880 - val_acc: 0.9200\n",
            "Epoch 378/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2018 - acc: 0.9344 - val_loss: 0.2875 - val_acc: 0.9200\n",
            "Epoch 379/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2098 - acc: 0.9335 - val_loss: 0.2909 - val_acc: 0.9164\n",
            "Epoch 380/500\n",
            "1098/1098 [==============================] - 0s 296us/sample - loss: 0.2174 - acc: 0.9281 - val_loss: 0.2982 - val_acc: 0.9091\n",
            "Epoch 381/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2007 - acc: 0.9362 - val_loss: 0.2770 - val_acc: 0.9345\n",
            "Epoch 382/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2153 - acc: 0.9317 - val_loss: 0.2906 - val_acc: 0.9236\n",
            "Epoch 383/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2069 - acc: 0.9299 - val_loss: 0.2983 - val_acc: 0.9200\n",
            "Epoch 384/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2127 - acc: 0.9326 - val_loss: 0.3035 - val_acc: 0.9018\n",
            "Epoch 385/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2047 - acc: 0.9326 - val_loss: 0.2768 - val_acc: 0.9273\n",
            "Epoch 386/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2014 - acc: 0.9299 - val_loss: 0.2707 - val_acc: 0.9382\n",
            "Epoch 387/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.1932 - acc: 0.9353 - val_loss: 0.3570 - val_acc: 0.8836\n",
            "Epoch 388/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2088 - acc: 0.9299 - val_loss: 0.2781 - val_acc: 0.9127\n",
            "Epoch 389/500\n",
            "1098/1098 [==============================] - 0s 271us/sample - loss: 0.2159 - acc: 0.9253 - val_loss: 0.2837 - val_acc: 0.9382\n",
            "Epoch 390/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2071 - acc: 0.9399 - val_loss: 0.2986 - val_acc: 0.9236\n",
            "Epoch 391/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2149 - acc: 0.9317 - val_loss: 0.2974 - val_acc: 0.9091\n",
            "Epoch 392/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2084 - acc: 0.9335 - val_loss: 0.2863 - val_acc: 0.9200\n",
            "Epoch 393/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2145 - acc: 0.9290 - val_loss: 0.2873 - val_acc: 0.9127\n",
            "Epoch 394/500\n",
            "1098/1098 [==============================] - 0s 303us/sample - loss: 0.2349 - acc: 0.9189 - val_loss: 0.2827 - val_acc: 0.9200\n",
            "Epoch 395/500\n",
            "1098/1098 [==============================] - 0s 298us/sample - loss: 0.2181 - acc: 0.9208 - val_loss: 0.2789 - val_acc: 0.9382\n",
            "Epoch 396/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2099 - acc: 0.9262 - val_loss: 0.3029 - val_acc: 0.9091\n",
            "Epoch 397/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2232 - acc: 0.9299 - val_loss: 0.2841 - val_acc: 0.9164\n",
            "Epoch 398/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2142 - acc: 0.9271 - val_loss: 0.2812 - val_acc: 0.9127\n",
            "Epoch 399/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2021 - acc: 0.9353 - val_loss: 0.2794 - val_acc: 0.9273\n",
            "Epoch 400/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2148 - acc: 0.9262 - val_loss: 0.2959 - val_acc: 0.9018\n",
            "Epoch 401/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2071 - acc: 0.9353 - val_loss: 0.2791 - val_acc: 0.9382\n",
            "Epoch 402/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2149 - acc: 0.9262 - val_loss: 0.2865 - val_acc: 0.9273\n",
            "Epoch 403/500\n",
            "1098/1098 [==============================] - 0s 275us/sample - loss: 0.2021 - acc: 0.9372 - val_loss: 0.2985 - val_acc: 0.9273\n",
            "Epoch 404/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.2035 - acc: 0.9353 - val_loss: 0.2808 - val_acc: 0.9236\n",
            "Epoch 405/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.1983 - acc: 0.9399 - val_loss: 0.2802 - val_acc: 0.9236\n",
            "Epoch 406/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2074 - acc: 0.9326 - val_loss: 0.3081 - val_acc: 0.9018\n",
            "Epoch 407/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2078 - acc: 0.9353 - val_loss: 0.2839 - val_acc: 0.9382\n",
            "Epoch 408/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2166 - acc: 0.9262 - val_loss: 0.2908 - val_acc: 0.9345\n",
            "Epoch 409/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2005 - acc: 0.9335 - val_loss: 0.2807 - val_acc: 0.9382\n",
            "Epoch 410/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2079 - acc: 0.9271 - val_loss: 0.3844 - val_acc: 0.9018\n",
            "Epoch 411/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2226 - acc: 0.9281 - val_loss: 0.2694 - val_acc: 0.9200\n",
            "Epoch 412/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2158 - acc: 0.9344 - val_loss: 0.2675 - val_acc: 0.9345\n",
            "Epoch 413/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2092 - acc: 0.9290 - val_loss: 0.2710 - val_acc: 0.9345\n",
            "Epoch 414/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2075 - acc: 0.9308 - val_loss: 0.2877 - val_acc: 0.9382\n",
            "Epoch 415/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2126 - acc: 0.9290 - val_loss: 0.2987 - val_acc: 0.9309\n",
            "Epoch 416/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2311 - acc: 0.9226 - val_loss: 0.3410 - val_acc: 0.9164\n",
            "Epoch 417/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2038 - acc: 0.9326 - val_loss: 0.2748 - val_acc: 0.9309\n",
            "Epoch 418/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2002 - acc: 0.9317 - val_loss: 0.2899 - val_acc: 0.9200\n",
            "Epoch 419/500\n",
            "1098/1098 [==============================] - 0s 306us/sample - loss: 0.2081 - acc: 0.9299 - val_loss: 0.2791 - val_acc: 0.9491\n",
            "Epoch 420/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.2056 - acc: 0.9344 - val_loss: 0.3072 - val_acc: 0.9309\n",
            "Epoch 421/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.1994 - acc: 0.9308 - val_loss: 0.3041 - val_acc: 0.9200\n",
            "Epoch 422/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2088 - acc: 0.9344 - val_loss: 0.2784 - val_acc: 0.9345\n",
            "Epoch 423/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2064 - acc: 0.9335 - val_loss: 0.2973 - val_acc: 0.9127\n",
            "Epoch 424/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.2054 - acc: 0.9362 - val_loss: 0.2827 - val_acc: 0.9273\n",
            "Epoch 425/500\n",
            "1098/1098 [==============================] - 0s 296us/sample - loss: 0.1939 - acc: 0.9372 - val_loss: 0.2825 - val_acc: 0.9236\n",
            "Epoch 426/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.1997 - acc: 0.9308 - val_loss: 0.2754 - val_acc: 0.9273\n",
            "Epoch 427/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.2013 - acc: 0.9262 - val_loss: 0.2951 - val_acc: 0.9164\n",
            "Epoch 428/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2152 - acc: 0.9290 - val_loss: 0.3137 - val_acc: 0.9127\n",
            "Epoch 429/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2217 - acc: 0.9235 - val_loss: 0.2894 - val_acc: 0.9164\n",
            "Epoch 430/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2329 - acc: 0.9199 - val_loss: 0.2752 - val_acc: 0.9236\n",
            "Epoch 431/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2093 - acc: 0.9281 - val_loss: 0.2757 - val_acc: 0.9273\n",
            "Epoch 432/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.1967 - acc: 0.9454 - val_loss: 0.2836 - val_acc: 0.9200\n",
            "Epoch 433/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2022 - acc: 0.9308 - val_loss: 0.3264 - val_acc: 0.9055\n",
            "Epoch 434/500\n",
            "1098/1098 [==============================] - 0s 286us/sample - loss: 0.2105 - acc: 0.9344 - val_loss: 0.2813 - val_acc: 0.9164\n",
            "Epoch 435/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2047 - acc: 0.9326 - val_loss: 0.2955 - val_acc: 0.9164\n",
            "Epoch 436/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2171 - acc: 0.9299 - val_loss: 0.2760 - val_acc: 0.9309\n",
            "Epoch 437/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.1992 - acc: 0.9399 - val_loss: 0.2781 - val_acc: 0.9273\n",
            "Epoch 438/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2092 - acc: 0.9262 - val_loss: 0.2893 - val_acc: 0.9200\n",
            "Epoch 439/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.2085 - acc: 0.9308 - val_loss: 0.2823 - val_acc: 0.9309\n",
            "Epoch 440/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.1974 - acc: 0.9335 - val_loss: 0.2878 - val_acc: 0.9309\n",
            "Epoch 441/500\n",
            "1098/1098 [==============================] - 0s 290us/sample - loss: 0.2007 - acc: 0.9317 - val_loss: 0.3115 - val_acc: 0.9091\n",
            "Epoch 442/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.2010 - acc: 0.9353 - val_loss: 0.2791 - val_acc: 0.9382\n",
            "Epoch 443/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.1859 - acc: 0.9381 - val_loss: 0.3071 - val_acc: 0.9091\n",
            "Epoch 444/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.1976 - acc: 0.9335 - val_loss: 0.2745 - val_acc: 0.9309\n",
            "Epoch 445/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.2089 - acc: 0.9253 - val_loss: 0.3147 - val_acc: 0.9127\n",
            "Epoch 446/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2005 - acc: 0.9362 - val_loss: 0.2950 - val_acc: 0.9127\n",
            "Epoch 447/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2106 - acc: 0.9308 - val_loss: 0.2859 - val_acc: 0.9200\n",
            "Epoch 448/500\n",
            "1098/1098 [==============================] - 0s 284us/sample - loss: 0.2147 - acc: 0.9271 - val_loss: 0.3001 - val_acc: 0.9200\n",
            "Epoch 449/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2120 - acc: 0.9262 - val_loss: 0.2877 - val_acc: 0.9236\n",
            "Epoch 450/500\n",
            "1098/1098 [==============================] - 0s 289us/sample - loss: 0.1937 - acc: 0.9435 - val_loss: 0.2765 - val_acc: 0.9382\n",
            "Epoch 451/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.2074 - acc: 0.9290 - val_loss: 0.2994 - val_acc: 0.9018\n",
            "Epoch 452/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.2132 - acc: 0.9235 - val_loss: 0.2798 - val_acc: 0.9273\n",
            "Epoch 453/500\n",
            "1098/1098 [==============================] - 0s 292us/sample - loss: 0.2002 - acc: 0.9290 - val_loss: 0.2773 - val_acc: 0.9091\n",
            "Epoch 454/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2039 - acc: 0.9299 - val_loss: 0.2834 - val_acc: 0.9200\n",
            "Epoch 455/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.2022 - acc: 0.9317 - val_loss: 0.2952 - val_acc: 0.9091\n",
            "Epoch 456/500\n",
            "1098/1098 [==============================] - 0s 274us/sample - loss: 0.2088 - acc: 0.9326 - val_loss: 0.2940 - val_acc: 0.9127\n",
            "Epoch 457/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2094 - acc: 0.9299 - val_loss: 0.2644 - val_acc: 0.9455\n",
            "Epoch 458/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2057 - acc: 0.9317 - val_loss: 0.2728 - val_acc: 0.9200\n",
            "Epoch 459/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.1955 - acc: 0.9281 - val_loss: 0.2793 - val_acc: 0.9200\n",
            "Epoch 460/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.1899 - acc: 0.9408 - val_loss: 0.2883 - val_acc: 0.9309\n",
            "Epoch 461/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2052 - acc: 0.9308 - val_loss: 0.2768 - val_acc: 0.9236\n",
            "Epoch 462/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.1958 - acc: 0.9344 - val_loss: 0.2892 - val_acc: 0.9200\n",
            "Epoch 463/500\n",
            "1098/1098 [==============================] - 0s 266us/sample - loss: 0.2073 - acc: 0.9290 - val_loss: 0.2901 - val_acc: 0.9055\n",
            "Epoch 464/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.1926 - acc: 0.9344 - val_loss: 0.3112 - val_acc: 0.9091\n",
            "Epoch 465/500\n",
            "1098/1098 [==============================] - 0s 285us/sample - loss: 0.2071 - acc: 0.9326 - val_loss: 0.3144 - val_acc: 0.9091\n",
            "Epoch 466/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.1985 - acc: 0.9372 - val_loss: 0.2993 - val_acc: 0.9164\n",
            "Epoch 467/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2100 - acc: 0.9281 - val_loss: 0.3387 - val_acc: 0.9055\n",
            "Epoch 468/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.1956 - acc: 0.9362 - val_loss: 0.2840 - val_acc: 0.9273\n",
            "Epoch 469/500\n",
            "1098/1098 [==============================] - 0s 267us/sample - loss: 0.2027 - acc: 0.9299 - val_loss: 0.2795 - val_acc: 0.9309\n",
            "Epoch 470/500\n",
            "1098/1098 [==============================] - 0s 279us/sample - loss: 0.2079 - acc: 0.9362 - val_loss: 0.3073 - val_acc: 0.9018\n",
            "Epoch 471/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.1945 - acc: 0.9381 - val_loss: 0.2800 - val_acc: 0.9200\n",
            "Epoch 472/500\n",
            "1098/1098 [==============================] - 0s 282us/sample - loss: 0.2013 - acc: 0.9381 - val_loss: 0.3109 - val_acc: 0.9055\n",
            "Epoch 473/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.1999 - acc: 0.9353 - val_loss: 0.3206 - val_acc: 0.9055\n",
            "Epoch 474/500\n",
            "1098/1098 [==============================] - 0s 299us/sample - loss: 0.1955 - acc: 0.9399 - val_loss: 0.2944 - val_acc: 0.9091\n",
            "Epoch 475/500\n",
            "1098/1098 [==============================] - 0s 314us/sample - loss: 0.1960 - acc: 0.9308 - val_loss: 0.2732 - val_acc: 0.9345\n",
            "Epoch 476/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2111 - acc: 0.9271 - val_loss: 0.2857 - val_acc: 0.9127\n",
            "Epoch 477/500\n",
            "1098/1098 [==============================] - 0s 276us/sample - loss: 0.2049 - acc: 0.9344 - val_loss: 0.3135 - val_acc: 0.9127\n",
            "Epoch 478/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2050 - acc: 0.9308 - val_loss: 0.3082 - val_acc: 0.9091\n",
            "Epoch 479/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.1927 - acc: 0.9408 - val_loss: 0.2909 - val_acc: 0.9200\n",
            "Epoch 480/500\n",
            "1098/1098 [==============================] - 0s 269us/sample - loss: 0.2127 - acc: 0.9326 - val_loss: 0.3080 - val_acc: 0.9164\n",
            "Epoch 481/500\n",
            "1098/1098 [==============================] - 0s 300us/sample - loss: 0.2125 - acc: 0.9281 - val_loss: 0.3377 - val_acc: 0.9018\n",
            "Epoch 482/500\n",
            "1098/1098 [==============================] - 0s 296us/sample - loss: 0.1971 - acc: 0.9381 - val_loss: 0.2733 - val_acc: 0.9345\n",
            "Epoch 483/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2012 - acc: 0.9262 - val_loss: 0.2892 - val_acc: 0.9273\n",
            "Epoch 484/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2019 - acc: 0.9299 - val_loss: 0.2804 - val_acc: 0.9200\n",
            "Epoch 485/500\n",
            "1098/1098 [==============================] - 0s 297us/sample - loss: 0.2224 - acc: 0.9262 - val_loss: 0.2879 - val_acc: 0.9236\n",
            "Epoch 486/500\n",
            "1098/1098 [==============================] - 0s 278us/sample - loss: 0.1961 - acc: 0.9317 - val_loss: 0.2688 - val_acc: 0.9200\n",
            "Epoch 487/500\n",
            "1098/1098 [==============================] - 0s 307us/sample - loss: 0.1866 - acc: 0.9353 - val_loss: 0.3122 - val_acc: 0.8945\n",
            "Epoch 488/500\n",
            "1098/1098 [==============================] - 0s 287us/sample - loss: 0.2144 - acc: 0.9271 - val_loss: 0.2802 - val_acc: 0.9309\n",
            "Epoch 489/500\n",
            "1098/1098 [==============================] - 0s 288us/sample - loss: 0.1985 - acc: 0.9317 - val_loss: 0.2777 - val_acc: 0.9236\n",
            "Epoch 490/500\n",
            "1098/1098 [==============================] - 0s 283us/sample - loss: 0.1800 - acc: 0.9399 - val_loss: 0.2846 - val_acc: 0.9091\n",
            "Epoch 491/500\n",
            "1098/1098 [==============================] - 0s 291us/sample - loss: 0.1918 - acc: 0.9326 - val_loss: 0.2917 - val_acc: 0.9200\n",
            "Epoch 492/500\n",
            "1098/1098 [==============================] - 0s 293us/sample - loss: 0.1922 - acc: 0.9362 - val_loss: 0.2913 - val_acc: 0.9273\n",
            "Epoch 493/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.1920 - acc: 0.9299 - val_loss: 0.3157 - val_acc: 0.9127\n",
            "Epoch 494/500\n",
            "1098/1098 [==============================] - 0s 273us/sample - loss: 0.2305 - acc: 0.9226 - val_loss: 0.2839 - val_acc: 0.9236\n",
            "Epoch 495/500\n",
            "1098/1098 [==============================] - 0s 280us/sample - loss: 0.2171 - acc: 0.9271 - val_loss: 0.2999 - val_acc: 0.8945\n",
            "Epoch 496/500\n",
            "1098/1098 [==============================] - 0s 281us/sample - loss: 0.1950 - acc: 0.9372 - val_loss: 0.3510 - val_acc: 0.9055\n",
            "Epoch 497/500\n",
            "1098/1098 [==============================] - 0s 272us/sample - loss: 0.2011 - acc: 0.9353 - val_loss: 0.2668 - val_acc: 0.9309\n",
            "Epoch 498/500\n",
            "1098/1098 [==============================] - 0s 300us/sample - loss: 0.1871 - acc: 0.9372 - val_loss: 0.2820 - val_acc: 0.9273\n",
            "Epoch 499/500\n",
            "1098/1098 [==============================] - 0s 301us/sample - loss: 0.1943 - acc: 0.9362 - val_loss: 0.2777 - val_acc: 0.9345\n",
            "Epoch 500/500\n",
            "1098/1098 [==============================] - 0s 277us/sample - loss: 0.1907 - acc: 0.9381 - val_loss: 0.2965 - val_acc: 0.9127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqMl5V5y0qeB",
        "outputId": "e4291c24-57b6-4069-d79b-87fc1fa683ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "plot_hist(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4XxTywAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgVoUlBKKRWgSUEppVTA/wPyBhBgJpEjOQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N14_-C010qeD",
        "outputId": "6702d389-3a1d-48d3-f4ed-fe9f5f7421c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "ts_x=np.array(ts_x).reshape(ts_x.shape[0],7,1)\n",
        "results1=model.evaluate(ts_x, ts_y, batch_size=32)\n",
        "print('\\nm=3, test=C\\n test loss: {}\\n  test acc: {}'.format(results1[0],results1[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138/138 [==============================] - 0s 956us/sample - loss: 0.3804 - acc: 0.8623\n",
            "\n",
            "m=3, test=C\n",
            " test loss: 0.38041798694842105\n",
            "  test acc: 0.8623188138008118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q9gBF63g0qeF",
        "outputId": "647098b3-32e5-4bb9-f4c1-793459137888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "ts_x2=np.array(ts_x2).reshape(ts_x2.shape[0],7,1)\n",
        "results2=model.evaluate(ts_x2, ts_y2, batch_size=32)\n",
        "print('\\nm=3, test=D\\n test loss: {}\\n  test acc: {}'.format(results2[0],results2[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104/104 [==============================] - 0s 688us/sample - loss: 0.1765 - acc: 0.9808\n",
            "\n",
            "m=3, test=D\n",
            " test loss: 0.17650621470350486\n",
            "  test acc: 0.9807692170143127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sFYFLr6u0qeH",
        "colab": {}
      },
      "source": [
        "## check where model did not predict correctly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5zto9F3RQL6",
        "colab_type": "code",
        "outputId": "7fa22b26-bfa5-4af0-c2f6-2b214591da5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "_, _, ts_x_df, ts_y_lbl, ts_x2_df, ts_y2_lbl = generate_tr_ts(df1=central, df2=site, m=3, method=None, h=3000, seed=2019, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f4c44fa2fe30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_x_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_y_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_x2_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_y2_lbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_tr_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcentral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: generate_tr_ts() got an unexpected keyword argument 'normalize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt61WyvfRQL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(encoder, model, test_x):\n",
        "    pred=model.predict(test_x)\n",
        "    return encoder.inverse_transform(np.round(pred))\n",
        "def return_predict(encoder, model, test_x, test_y):\n",
        "    pred=prediction(encoder, model, test_x)\n",
        "    results=[pred==true for pred, true in list(zip(pred,test_y))]\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-sA6a81RQL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_x_df['Label']=ts_y_lbl['TRGRESP']\n",
        "ts_x_df['Pred']=prediction(encoder, model, ts_x)\n",
        "ts_x_df['Correct']=return_predict(encoder, model, ts_x, ts_y_lbl['TRGRESP'])\n",
        "ts_x_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMav1ZBNRQMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_x2_df['Label']=ts_y2_lbl['TRGRESP']\n",
        "ts_x2_df['Pred']=prediction(encoder, model, ts_x2)\n",
        "ts_x2_df['Correct']=return_predict(encoder, model, ts_x2, ts_y2_lbl['TRGRESP'])\n",
        "ts_x2_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}